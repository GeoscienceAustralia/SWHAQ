"""
iterate_vulnerability_scenario.py - run an impact scenario multiple times,
    randomly selecting eligible buildings for retrofit each time.

Runs the damage calculation from HazImp 100 times. Each time, select a random
set of buildings to be assigned new (retrofit) vulnerability functions.

The hazard data, exposure data and vulnerability files are read once to save
processing time (see lines with `the_pipeline.jobs`).

At the end, store the results of each iteration, and calculate 5, 10, 50, 90
and 95 percentiles based on the iterations. Join the percentiles back onto the
other exposure attributes so we can aggregate, etc.

NOTE:
This script is specifically written for use in a PBS job on gadi.nci.org.au.
Modifications would be required to make it work elsewhere.

Author: Craig Arthur
Date: 2022-11-30

"""


import os
import sys

import warnings
import logging
import pandas as pd
import numpy as np

from hazimp import config, context
from hazimp import pipeline
from hazimp.aggregate import aggregate_loss_atts

warnings.simplefilter(action="ignore", category=FutureWarning)
logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s: %(name)s: %(levelname)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S')

LOGGER = logging.getLogger()

eventid = sys.argv[1]
niterations = 100

LOGGER.info(f"Running event {eventid} {niterations} times")
# Set up configuration file to use the following:
# vulnerability_method: normal
# No tabulation
# No aggregation
PBS_JOBFS = os.environ.get("PBS_JOBFS")
exppath = "/g/data/w85/QFES_SWHA/exposure/2022"
hazardpath = f"/g/data/w85/QFES_SWHA/wind/local/{eventid}"
outputpath = f"/g/data/w85/QFES_SWHA/impact/RETROFIT/{eventid}"
config_file_template = open("/g/data/w85/QFES_SWHA/configuration/hazimp/iterate_hazimp.template.yaml").read()
event_config_path = f"/g/data/w85/QFES_SWHA/configuration/hazimp/iterate_hazimp.{eventid}.yaml"

event_config = config_file_template.replace('EVENTID', eventid)
with open(event_config_path, 'w') as fout:
    fout.write(event_config)

if not os.path.isdir(outputpath):
    os.makedirs(outputpath)

# Additional eligibility criteria - presently SA2 level ADRI quartiles:
eligibilitypath = "/g/data/w85/QFES_SWHA/exposure/2022"
eligibilityfile = "targeted_retrofit_eligibility.csv"
eligibilitydf = pd.read_csv(os.path.join(eligibilitypath, eligibilityfile),
                            index_col='SA2_MAIN16')

# Useful column names:
WVC = 'WIND_VULNERABILITY_FUNCTION_ID'
ACC = 'AS4055_CLASS'
TRC = 'Targeted_Retrofit'

# Legacy buildings original vuln functions: retrofit functions
old_mapping = {
    'dw350': 'dw650',
    'dw351': 'dw651',
    'dw352': 'dw652',
}

# modern buildings original vuln functions: retrofit functions (windows)
new_mapping = {
    'dw353': 'dw453',
    'dw354': 'dw454',
    'dw355': 'dw455',
    'dw356': 'dw456',
    'dw357': 'dw457',
    'dw358': 'dw458',
    'dw359': 'dw459',
    'dw360': 'dw460',
    'dw361': 'dw461',
    'dw362': 'dw462',
    'dw363': 'dw463',
    'dw364': 'dw464',
}


def permute_retrofit(expdf, eligibilitydf, mapping, frac,
                     eligibilitykey="SA2_CODE"):
    """
    Randomly select eligible buildings and update with a retrofit vulnerability
    function.

    You may need to change the percentages of new and old buildings that are
    eligible (see the `frac` variable). Additional eligibility criteria
    provided in the `eligibilitydf` `pd.DataFrame`

    :param expdf: :class:`pd.DataFrame` of exposure assets
    :param eligibilitydf: :class:`pd.DataFrame` containing eligibility criteria
        at some geographic resolution (or some attribute of the exposure data)
    :param dict mapping: 1-to-1 mapping of vulnerability functions to swap
    :param float frac: Fraction of buildings to be updated
    :param eligibilitykey: `str` or `list` of attributes that will be used to
        join the eligibility data to the exposure data. NOTE: This is assumed
        to be the index of the `eligibilitydf` `pd.DataFrame`.

    """
    # Set some aliases for column names to make the code easier to read
    if TRC in expdf.columns:
        expdf.drop(TRC, axis='columns', inplace=True)
    df_retro = pd.merge(expdf, eligibilitydf[[TRC]],
                        left_on=[eligibilitykey],
                        right_index=True,
                        how="outer",
                        sort=False)
    df_retro = df_retro.sort_values(TRC).drop_duplicates('internal_id')
    df_retro = df_retro[~df_retro[TRC].isna()]

    # Following two are presently fixed - would need additional arguments to
    # this function call.
    rc1 = ["Eligible"]
    rc2 = ["N3", "N4"]
    rc3 = list(mapping.keys())

    selectdf = df_retro[(df_retro[TRC].isin(rc1)) &
                        (df_retro[ACC].isin(rc2)) &
                        (df_retro[WVC].isin(rc3))]

    selectdf = selectdf.sample(frac=(frac))
    selectdf[WVC] = selectdf[WVC].map(mapping)
    df_retro.update(selectdf)
    df_retro.drop_duplicates('internal_id', inplace=True)
    nretrofitted = df_retro[WVC].isin(list(mapping.values())).sum()
    LOGGER.info(f"Number of retrofitted buildings: {nretrofitted}")
    return df_retro


config_list = config.read_config_file(event_config_path)

# Change the output of HazImp to go to the PBS JOB filesystem on gadi (remove
# if not running on gadi!)
config_list[-1]['save'] = f"{PBS_JOBFS}/output/iteration.csv"
# Change the input hazard file:
hazard_file = os.path.join(hazardpath, "local_wind.tif")
LOGGER.info(f"Hazard file: {hazard_file}")
config_list[3]['hazard_raster']['file_list'] = hazard_file
colnames = [f"{i:03d}" for i in range(100)]
colnames = ['internal_id', 'LID']
outdf = pd.DataFrame(columns=colnames)

cont_in = context.Context()
# TODO: Make the entity name a tailored variable.
cont_in.set_prov_label("probabilistic_retrofit_scenario_analysis")
LOGGER.info("Building instance")
calc_jobs = config.instance_builder(config_list)
LOGGER.info("Building the pipeline")
the_pipeline = pipeline.PipeLine(calc_jobs)

# Initial pipeline to load the exposure data, vulnerability and raster data
LOGGER.info("Running initial pipeline")
the_pipeline.jobs[0](cont_in)
the_pipeline.jobs[1](cont_in)
the_pipeline.jobs[2](cont_in)

# This is not the ideal way to handle this issue, but want to ensure the
# SA1 codes and others are integers in the output
CODECOLS = ['MB_CODE', 'SA1_CODE', 'SA2_CODE', 'LGA_CODE']
cont_in.exposure_att[CODECOLS] = cont_in.exposure_att[CODECOLS].astype('int64')

# This should report zero!
nretro = cont_in.exposure_att[WVC].isin(['dw650', 'dw651', 'dw652']).sum()
LOGGER.info(f"Number of retrofitted buildings prior to processing: {nretro}")
# Retain the unmodified exposure attributes. Otherwise, we will
# progressively retrofit more and more buildings.
base_exp_att = cont_in.exposure_att.copy(deep=True)

outdf[['internal_id', 'LID']] = cont_in.exposure_att[['internal_id', 'LID']]
outdf.set_index('internal_id', inplace=True)

for iteration in range(niterations):
    LOGGER.info(f"Running iteration #{iteration}")

    # Following lines should run the permutation for separate mappings
    # e.g. older buidings and newer buildings.
    tmpexpdf = permute_retrofit(base_exp_att, eligibilitydf,
                                old_mapping, frac=0.48875)
    # tmpexpdf = permute_retrofit(tmpexpdf, eligibilitydf,
    #                            new_mapping, frac=0.113)

    cont_in.exposure_att.loc[tmpexpdf.index, WVC] = tmpexpdf[WVC]

    # Run the calculation parts of the pipeline:
    the_pipeline.jobs[3](cont_in)
    the_pipeline.jobs[4](cont_in)
    the_pipeline.jobs[5](cont_in)
    the_pipeline.jobs[6](cont_in)

    # Grab the structural loss ratio - this is all we're actually interested in
    outdf = outdf.merge(
        cont_in.exposure_att.set_index('internal_id')['structural'],
        how="inner", left_index=True, right_index=True)
    outdf.rename(columns={'structural': f"{iteration:03d}"}, inplace=True)


the_pipeline.jobs[7](cont_in)
the_pipeline.jobs[8](cont_in)

outputfile = f"{outputpath}/iterations.csv"
outdf.merge(cont_in.exposure_att.set_index('internal_id'), how="inner",
            left_index=True, right_index=True).\
                to_csv(outputfile, float_format="%.5f")
quantiles = outdf.quantile(q=[0.05, 0.1, 0.5, 0.9, 0.95], axis=1).T
quantiles.columns = [str(col) for col in quantiles.columns]
quantiles = quantiles.join(cont_in.exposure_att.set_index('internal_id'),)
quantiles.to_csv(f"{outputpath}/quantiles.csv")
kwargs = {'0.05': ['mean', 'max'], '0.1': ['mean', 'max'],
          '0.5': ['mean', 'max'], '0.9': ['mean', 'max'],
          '0.95': ['mean', 'max']}
aggdf = aggregate_loss_atts(quantiles, groupby='SA1_CODE', kwargs=kwargs)
aggdf.to_csv(os.path.join(outputpath, "SA1_quantiles.csv"),
             float_format="%.5f")
aggdf = aggregate_loss_atts(quantiles, groupby='LGA_CODE', kwargs=kwargs)
aggdf.to_csv(os.path.join(outputpath, "LGA_quantiles.csv"),
             float_format="%.5f")
