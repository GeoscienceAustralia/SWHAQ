"""
iterate_vulnerability_scenario.py - run an impact scenario multiple times,
    randomly selecting eligible buildings for retrofit each time.

Runs the damage calculation from HazImp 100 times. Each time, select a random
set of buildings to be assigned new (retrofit) vulnerability functions.

The hazard data, exposure data and vulnerability files are read once to save
processing time (see lines with `the_pipeline.jobs`).

At the end, store the results of each iteration, and calculate 5, 10, 50, 90
and 95 percentiles based on the iterations. Join the percentiles back onto the
other exposure attributes so we can aggregate, etc.

NOTE:
This script is specifically written for use in a PBS job on gadi.nci.org.au.
Modifications would be required to make it work elsewhere.

Author: Craig Arthur
Date: 2022-11-29

"""


import os
import sys

import warnings
import logging
import pandas as pd
import numpy as np

from hazimp import config, context
from hazimp import pipeline
from hazimp.aggregate import aggregate_loss_atts

warnings.simplefilter(action="ignore", category=FutureWarning)
logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s: %(name)s: %(levelname)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S')

LOGGER = logging.getLogger()

eventid = sys.argv[1]
# Set up configuration file to use the following:
# vulnerability_method: normal
# No tabulation
# No aggregation
PBS_JOBFS = os.environ.get("PBS_JOBFS")
exppath = "/g/data/w85/QFES_SWHA/exposure/2022"
hazardpath = f"/g/data/w85/QFES_SWHA/wind/local/{eventid}"
outputpath = f"/g/data/w85/QFES_SWHA/impact/RETROFIT/{eventid}"
config_file = "/g/data/w85/QFES_SWHA/configuration/hazimp/iterate_hazimp_004_08495.yaml"

# Additional eligibility criteria - presently SA2 level ADRI quartiles:
eligibilitypath = "/g/data/w85/QFES_SWHA/exposure/2022"
eligibilityfile = "targeted_retrofit_eligibility.csv"
eligibilitydf = pd.read_csv(os.path.join(eligibilitypath, eligibilityfile),
                            index_col='SA2_MAIN16')

# Useful column names:
WVC = 'WIND_VULNERABILITY_FUNCTION_ID'
ACC = 'AS4055_CLASS'
TRC = 'Targeted_Retrofit'

def permute_retrofit(df, eligibilitydf):
    """
    Randomly select eligible buildings and update with a retrofit vulnerability
    function.

    You may need to change the percentages of new and old buildings that are
    eligible (see the `retrofit_percent_new` and `retrofit_percent_old`
    variables). Additional eligibility criteria provided in the
    `eligibilitydf` `pd.DataFrame`


    """
    # Set some aliases for column names to make the code easier to read
    if TRC in df.columns:
        df.drop(TRC, axis='columns', inplace=True)
    df_retro = pd.merge(df, eligibilitydf[[TRC]],
                        left_on=["SA2_CODE"],
                        right_index=True,
                        how="outer",
                        sort=False)
    df_retro = df_retro.sort_values(TRC).drop_duplicates('internal_id')
    df_retro = df_retro[~df_retro[TRC].isna()]

    # Overall fraction of buildings to retrofit:
    retrofit_percent_new = 0.113 # New builds (if included in eligibility)
    retrofit_percent_old = 0.48875 # Legacy buildings (here assumed 85% of owner-occupied, which is 57.5% of all homes)

    # Legacy buildings original vuln functions: retrofit functions
    dic_old_RF3 = {
        'dw350': 'dw650',
        'dw351': 'dw651',
        'dw352': 'dw652',
    }

    # modern buildings original vuln functions: retrofit functions (windows)
    dic_new = {
        'dw353': 'dw453',
        'dw354': 'dw454',
        'dw355': 'dw455',
        'dw356': 'dw456',
        'dw357': 'dw457',
        'dw358': 'dw458',
        'dw359': 'dw459',
        'dw360': 'dw460',
        'dw361': 'dw461',
        'dw362': 'dw462',
        'dw363': 'dw463',
        'dw364': 'dw464',
    }

    rc1 = ["Eligible"]
    rc2 = ["N3", "N4"]
    list_old_RF3 = list(dic_old_RF3.keys())
    #list_new = list(dic_new.keys())
    # find relevent buildings and sample a percent of them (retrofit_percent)
    #df_new = df_retro[(df_retro[TRC].isin(rc1)) &
    #                  (df_retro[ACC].isin(rc2)) &
    #                  (df_retro[WVC].isin(list_new))]
    #df_new = df_new.sample(frac=retrofit_percent_new)
    df_old_RF3 = df_retro[(df_retro[TRC].isin(rc1)) &
                          (df_retro[ACC].isin(rc2)) &
                          (df_retro[WVC].isin(list_old_RF3))]
    df_old_RF3 = df_old_RF3.sample(frac=(retrofit_percent_old))
    #df_new[WVC] = df_new[WVC].map(dic_new)
    df_old_RF3[WVC] = df_old_RF3[WVC].map(dic_old_RF3)
    #df_retro.update(df_new)
    df_retro.update(df_old_RF3)
    df_retro.drop_duplicates('internal_id', inplace=True)
    return df_retro

config_list = config.read_config_file(config_file)

# Change the output of HazImp to go to the PBS JOB filesystem on gadi (remove if
# not running on gadi!)
config_list[-1]['save'] = f"{PBS_JOBFS}/output/iteration.csv"
# Change the input hazard file:
hazard_file = os.path.join(hazardpath, "local_wind.tif")
LOGGER.info(f"Hazard file: {hazard_file}")
config_list[3]['hazard_raster']['file_list'] = hazard_file
colnames = [f"{i:03d}" for i in range(100)]
colnames=['internal_id', 'LID']
outdf = pd.DataFrame(columns=colnames)

cont_in = context.Context()
# TODO: Make the entity name a tailored variable.
cont_in.set_prov_label("probabilistic_retrofit_scenario_analysis")
LOGGER.info("Building instance")
calc_jobs = config.instance_builder(config_list)
LOGGER.info("Building the pipeline")
the_pipeline = pipeline.PipeLine(calc_jobs)

#Initial pipeline to load the exposure data, vulnerability and raster data
LOGGER.info("Running initial pipeline")
the_pipeline.jobs[0](cont_in)
the_pipeline.jobs[1](cont_in)
the_pipeline.jobs[2](cont_in)

# This is not the ideal way to handle this issue, but want to ensure the
# SA1 codes and others are integers in the output
CODECOLS = ['MB_CODE', 'SA1_CODE', 'SA2_CODE', 'LGA_CODE']
cont_in.exposure_att[CODECOLS] = cont_in.exposure_att[CODECOLS].astype('int64')

# This should report zero!
LOGGER.info(f"Number of retrofitted buildings: {cont_in.exposure_att[WVC].isin(['dw650','dw651','dw652']).sum()}")
# Retain the unmodified exposure attributes. Otherwise, we will
# progressively retrofit more and more buildings.
base_exp_att = cont_in.exposure_att.copy(deep=True)

outdf[['internal_id', 'LID']] = cont_in.exposure_att[['internal_id', 'LID']]
outdf.set_index('internal_id', inplace=True)

for iteration in range(100):
    LOGGER.info(f"Running iteration #{iteration}")

    # Update the retrofitted buildings in the exposure atts
    tmpexpdf = permute_retrofit(base_exp_att, eligibilitydf)
    cont_in.exposure_att.loc[tmpexpdf.index, WVC] = tmpexpdf[WVC]
    LOGGER.info(f"Number of retrofitted buildings: {cont_in.exposure_att[WVC].isin(['dw650','dw651','dw652']).sum()}")

    # Run the calculation parts of the pipeline:
    the_pipeline.jobs[3](cont_in)
    the_pipeline.jobs[4](cont_in)
    the_pipeline.jobs[5](cont_in)
    the_pipeline.jobs[6](cont_in)

    # Grab the structural loss ratio - this is all we're actually interested in
    outdf = outdf.merge(cont_in.exposure_att.set_index('internal_id')['structural'],
                        how="inner", left_index=True, right_index=True)
    outdf.rename(columns={'structural': f"{iteration:03d}"}, inplace=True)


the_pipeline.jobs[7](cont_in)
the_pipeline.jobs[8](cont_in)

outputfile = f"{outputpath}/iterations.csv"
outdf.merge(cont_in.exposure_att.set_index('internal_id'), how="inner", left_index=True, right_index=True).to_csv(outputfile, float_format="%.5f")
quantiles = outdf.quantile(q=[0.05, 0.1, 0.5, 0.9, 0.95], axis=1).T
quantiles.columns = [str(col) for col in quantiles.columns]
quantiles = quantiles.join(cont_in.exposure_att.set_index('internal_id'),)
quantiles.to_csv(f"{outputpath}/quantiles.csv")
kwargs = {'0.05': ['mean', 'max'], '0.1': ['mean', 'max'], '0.5': ['mean', 'max'], '0.9': ['mean', 'max'], '0.95': ['mean', 'max']}
aggdf = aggregate_loss_atts(quantiles, groupby='SA1_CODE', kwargs=kwargs)
aggdf.to_csv(os.path.join(outputpath, "SA1_quantiles.csv"), float_format="%.5f")
aggdf = aggregate_loss_atts(quantiles, groupby='LGA_CODE', kwargs=kwargs)
aggdf.to_csv(os.path.join(outputpath, "LGA_quantiles.csv"), float_format="%.5f")